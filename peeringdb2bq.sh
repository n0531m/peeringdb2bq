#!/bin/bash -v

##root directory for downloaded an temporarly files
DIR_DATA=./data

DIR_RAW=$DIR_DATA/raw
DIR_NEWLINE_DELIMITED=$DIR_DATA/newlinedelimited
DIR_META=$DIR_DATA/meta

#create local data directory if not existing yet.
for dir in $DIR_DATA $DIR_RAW $DIR_NEWLINE_DELIMITED $DIR_META
do
	if test ! -d $dir
		then mkdir $dir
	fi
done

##your project
GCP_PROJECTID=moritani-bigdata

gcloud config set project $GCP_PROJECTID

##your Google Cloud Storage bucket for temporal data uploads
GCS_BUCKET=gs://moritani-bigdata
#path in the bucket above
GCS_DATA_ROOT=$GCS_BUCKET/peeringdb2bq
GCS_DATA_DIR=$GCS_DATA_ROOT/newlinedelimited

##BigQuery. dataset to put the tables in 
BQ_DATASET=peeringdb
bq mk $BQ_DATASET



#PeeringDB API
PEERINGD_API_ENDPOINT=https://www.peeringdb.com/api

##depth of nested information to be returned.
#DEPTH=0 #default
#DEPTH=1 #nested anchors
DEPTH=2 #actual objects

for obj in fac ix ixfac ixlan ixpfx net netfac netixlan org poc
do
	if test -e $DIR_RAW/$obj
		then ZFLAG="-z $DIR_RAW/$obj"
	else ZFLAG=
	fi
	curl -o $DIR_RAW/$obj.json $ZFLAG $PEERINGD_API_ENDPOINT/$obj?depth=$DEPTH
	#conver to newline-delimited json files
	cat $DIR_RAW/$obj.json | jq -c -S  ".data[]" > $DIR_NEWLINE_DELIMITED/$obj.json
	#keep meta data
	cat $DIR_RAW/$obj.json | jq ".meta" > $DIR_META/$obj.json 
done

#upload data to GCS for injestion
gsutil -m cp $DIR_NEWLINE_DELIMITED/* $GCS_DATA_DIR/

for obj in fac ix ixfac ixlan ixpfx net netfac netixlan org poc
do
	#clear existing tables
	bq rm -f $BQ_DATASET.$obj
	#loading data with autodetection of schema (to make is simpler if not optimal)
	bq load --autodetect --source_format NEWLINE_DELIMITED_JSON $BQ_DATASET.$obj $GCS_DATA_DIR/$obj.json
	description="generated by $0. data extracted from peeringdb : $(cat $DIR_META/$obj.json | jq .generated)"
	bq update --description "$description" $BQ_DATASET.$obj 
done







